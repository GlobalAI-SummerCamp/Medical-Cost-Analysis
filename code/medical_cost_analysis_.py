# -*- coding: utf-8 -*-
"""Medical-Cost-Analysis .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hEVAbYYgJGfJiF7iZOKP6DdWhiCUYeKK

## Step 2

#### Importing Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn.model_selection 
import sklearn.metrics 
import sklearn.linear_model
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv("insurance.csv")

"""## Step 3"""

df.head(5) #First 5 rows are shown below

df.shape #number of column and rows of df;

df.info()  #general informations about df

"""As we can see, we have several d-types features. *'Object'* d-type features are *string* and categorical values."""

df.isnull().sum() #Null values are checked

pd.options.display.float_format='{:.3f}'.format
df.describe()  # We should check the statistical summary about each column

"""### Exploratory Data Analysis"""

#Distribution of bmi
plt.figure(figsize=(12,5))
sns.distplot(df['bmi'])
plt.title('BMI Distribution')
plt.show()

"""
Category	BMI range - kg/m2

*   Severe Thinness	< 16
*   Moderate Thinness	16 - 17
*   Mild Thinness	17 - 18.5
*   Normal	18.5 - 25
*   Overweight	25 - 30
*   Obese Class I	30 - 35
*   Obese Class II	35 - 40
*   Obese Class III	> 40

According to this data, more than half of people are obese."""

f= plt.figure(figsize=(12,5))
ax=f.add_subplot(121)
sns.distplot(df[(df.smoker == 'yes')]["charges"],color='c',ax=ax)
ax.set_title('Distribution of charges for smokers')

ax=f.add_subplot(122)
sns.distplot(df[(df.smoker == 'no')]["charges"],color='g',ax=ax)
ax.set_title('Distribution of charges for non-smokers')

"""Smoking people spend more money for medical treatment than non-smoking people."""

sns.catplot(x="region", kind="count",hue = 'smoker', palette="pink", data=df)

"""Number of non-smoking people are greater than smoking people. Also, southeast region has the most smokers."""

sns.displot(df, x="bmi", col="sex")

"""Under 30  BMI is almost same, but unhealthy people, have bigger than 30 BMI are mostly men.
Clear difference starts at 50 and more bmi
"""

sns.catplot(x='region', kind="count",hue = 'children', palette="pink", data=df)

regions = list(df['region'].unique())
Dict = {}
for i in regions:
  count = df[(df.region == '{0}'.format(i))]['children'].sum()
  Dict['{0}'.format(i)] = count

max = 0
for a,b in Dict.items():
  if b > max:
    max = b
    name = a
print(name,max)

"""Most of the people have no child.
There are few people have 4 and 5 children.
"""

plt.figure(figsize=(10,6))
ax = sns.stripplot(x='age',y='bmi',data=df)
ax.set_title('Strip plot of Age and BMI')
ax.set_xlabel('Age')
ax.set_ylabel('BMI')

"""Obesity is more common in younger people.

Elder people have least variance of bmi


"""

plt.figure(figsize=(10,6))
ax = sns.stripplot(x='children',y='bmi',data=df)
ax.set_title('Scatter plot of Children and BMI')
ax.set_xlabel('Children')
ax.set_ylabel('BMI')

"""The people who have fewer children more prone to being obese"""

plt.figure(figsize=(12,5))
plt.title("Box plot for BMI")
sns.boxplot(df['bmi'])

"""Average bmi is between 25-35. But, there are small number of people have 45 bmi and over. It is a outlier."""

plt.figure(figsize=(10,6))
ax = sns.scatterplot(x='bmi',y='charges',data=df,palette='magma',hue='smoker')
ax.set_title('Scatter plot of charges and bmi')

plt.figure(figsize=(10,6))
sns.lmplot(x="bmi", y="charges", hue="smoker", data=df, palette = 'magma', size = 8)

"""The people who have bigger than 30 bmi prone"""

ax = sns.barplot(x="region", y="bmi", hue="smoker", data=df)

"""According to BMI there is no differences between smokers and non-smokers.

Most BMI is in the Southeast region.

## Step 4 Veri ön işlemesi
"""

df.columns

"""Firstly, we have to reorder columns, because of encoding. When we apply one-hot-encoding to features, encoded features are appended at the end of the dataframe"""

#reordering columns
df = df[['charges','age','bmi','children','sex','smoker','region']]

df.head()

#We want to seperate real values and encoded values
df_encoded = df.copy()

df.head()

# from sklearn.preprocessing import LabelEncoder
# #sex
# le = LabelEncoder()
# le.fit(df_encoded.sex.drop_duplicates()) 
# df_encoded.sex = le.transform(df_encoded.sex)
# # smoker or not
# le.fit(df_encoded.smoker.drop_duplicates()) 
# df_encoded.smoker = le.transform(df_encoded.smoker)
# #region
# le.fit(df_encoded.region.drop_duplicates()) 
# df_encoded.region = le.transform(df_encoded.region)

# from sklearn.preprocessing import OneHotEncoder
# ohe = OneHotEncoder()
# categorical_cols = ['sex', 'smoker', 'region']
# #One-hot-encode the categorical columns.
# #Unfortunately outputs an array instead of dataframe.
# array_hot_encoded = ohe.fit_transform(df[categorical_cols])

# #Convert it to df
# data_hot_encoded = pd.DataFrame(array_hot_encoded, index=df.index)

# #Extract only the columns that didnt need to be encoded
# data_other_cols = df.drop(columns=categorical_cols)

# #Concatenate the two dataframes : 
# df_encoded = pd.concat([data_hot_encoded, data_other_cols], axis=1)

categorical_cols = ['sex','smoker','region']
df_encoded= pd.get_dummies(df, columns=categorical_cols)

"""There are several types of encoding methods : 
*   Label Encoding
*   One-Hot Encoding(aka pd.get_dummies)

**Label Encoding:** It is used for target values,which are y. However, if you are lazy to use one hot encoding you can also use it.

**One-Hot Encoding:** Also known as Dummy Encoding, One-Hot Encoding is a method to encode categorical variables, where no such ordinal relationship exists, to numerical data that Machine Learning algorithms can deal with.One hot encoding creates new, binary columns, indicating the presence of each possible value from the original data. These columns store ones and zeros for each row, indicating the categorical value of that row.





"""

df_encoded

X = df_encoded.iloc[:, 1:].values
y = df_encoded.iloc[:, 0].values

"""Features and target values are seperated."""

X

y

# train and test sets are created
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)

X_train

y_train.shape

"""As you see, y_train set is a vector.We should reshape it to process """

y_train

y_train.ndim

y_train = y_train.reshape(-1, 1)

y_train.shape

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
sc_y = StandardScaler()
X_train[:, 0:2] = sc_X.fit_transform(X_train[:, 0:2])
y_train = sc_y.fit_transform(y_train)

X_test[:,0:2] = sc_X.transform(X_test[:,0:2])
# from sklearn.preprocessing import StandardScaler
# sc = StandardScaler()
# X_train[:, 0:2] = sc.fit_transform(X_train[:, 0:2])
# X_test[:, 0:2] = sc.transform(X_test[:, 0:2])

"""**Standardization** of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance."""

X_train

X_test

y_train

"""## Step 5

#### Model egitim
"""

from sklearn.tree import DecisionTreeRegressor
decision_tree = DecisionTreeRegressor(random_state = 0)
decision_tree.fit(X_train, y_train)

from sklearn.ensemble import RandomForestRegressor
random_forest = RandomForestRegressor(n_estimators = 10, random_state = 0)
random_forest.fit(X_train, y_train)

from sklearn.svm import SVR
svr = SVR(kernel = 'rbf')
svr.fit(X_train, y_train)

"""#### Cross validation"""

def calculate_cross_val(regressor):
  from sklearn.model_selection import cross_val_score
  scores = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10)
  print("Score: {:.2f} %".format(scores.mean()*100))
  print("Standard Deviation: {:.2f} %".format(scores.std()*100))

print("----Decision Tree----")
calculate_cross_val(decision_tree)
print("----Random Forest----")
calculate_cross_val(random_forest)
print("---------SVR---------")
calculate_cross_val(svr)

"""It is clearly defined decision tree is the worst model for this data. For now, we may use both random forest and svr. But, We'll see...

### Prediction

Some preprocessing for prediction
"""

X_test.shape

X_test[:5]

X_test.ndim

(svr.predict(X_test)).shape

(svr.predict(X_test)).reshape(-1, 1).shape

y_pred_svr = sc_y.inverse_transform((svr.predict(X_test)).reshape(-1, 1))
y_pred_decision_tree = sc_y.inverse_transform((decision_tree.predict(X_test)).reshape(-1, 1))
y_pred_random_forest = sc_y.inverse_transform((random_forest.predict(X_test)).reshape(-1, 1))

"""Let's see approximations of our model to test values"""

np.set_printoptions(precision=2)
print(np.concatenate((y_pred_svr.reshape(len(y_pred_svr),1), y_test.reshape(len(y_test),1)),1)[:5])

"""#### GridSearch"""

from sklearn.model_selection import GridSearchCV
parameters_svr = [{'C': [0.25, 0.5, 0.75, 1], 'kernel': ['linear']},
                  {'C': [0.25, 0.5, 0.75, 1], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]
grid_search_svr = GridSearchCV(estimator = svr,
                           param_grid = parameters_svr,
                           scoring = 'neg_mean_squared_error',
                           cv = 10,
                           n_jobs = -1)
grid_search_svr.fit(X_train, y_train)

# Define Parameters
parameters_random_forest = { 
    'n_estimators': [75,100,125,150],
    'max_features': ['auto', 'sqrt', 'log2'],
    'criterion' :['squared_error', 'absolute_error', 'poisson']
}
grid_search_random_forest = GridSearchCV(estimator = random_forest,
                           param_grid = parameters_random_forest,
                           scoring = 'neg_mean_squared_error',
                           cv = 10,
                           n_jobs = -1)

grid_search_random_forest.fit(X_train, y_train)

# Define Parameters
parameters_decision_tree = { 
    'max_features': ['auto', 'sqrt', 'log2'],
    'splitter': ['best', 'random'],
    'criterion' :['squared_error', 'friedman_mse', 'absolute_error', 'poisson']
}
grid_search_decision_tree = GridSearchCV(estimator = decision_tree,
                           param_grid = parameters_decision_tree,
                           scoring = 'neg_mean_squared_error',
                           cv = 10,
                           n_jobs = -1)

grid_search_decision_tree.fit(X_train, y_train)

"""### Metrics"""

def grid_search_metric(grid_search):
  best_accuracy = grid_search.best_score_
  best_parameters = grid_search.best_params_
  print("MSE: {:.2f}".format(np.abs(best_accuracy*100)))
  print("Best Parameters:", best_parameters)

from sklearn.metrics import mean_squared_error,r2_score,max_error
print("Mean ")
print("SVR : ",mean_squared_error(y_test, y_pred_svr))
print("Random Forest : ",mean_squared_error(y_test, y_pred_random_forest))
print("Decision Tree : ",mean_squared_error(y_test, y_pred_decision_tree))
print("---------------------")
print("R2_score")
print("SVR : ",r2_score(y_test, y_pred_svr))
print("Random Forest : ",r2_score(y_test, y_pred_random_forest))
print("Decision Tree : ",r2_score(y_test, y_pred_decision_tree))

print("Grid Search SVR")
grid_search_metric(grid_search_svr)
print("Grid Search Random Forest")
grid_search_metric(grid_search_random_forest)
print("Grid Search Decision Tree")
grid_search_metric(grid_search_decision_tree)

"""As a result of, Random Forest is best model for this data."""